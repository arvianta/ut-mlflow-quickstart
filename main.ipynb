{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c0e0e1-d29e-489b-9adc-8b0e03c4f608",
   "metadata": {},
   "source": [
    "## Project Workflow Overview\n",
    "\n",
    "### 1. **Import Libraries**\n",
    "   - Load all necessary libraries, including MLflow, data processing tools, and model frameworks.\n",
    "\n",
    "### 2. **Set MLflow Tracking URI**\n",
    "   - Configure the URI to track the experiment runs in your MLflow server or a local directory.\n",
    "\n",
    "### 3. **Set MLflow Experiment**\n",
    "   - Define or create the MLflow experiment where all runs will be logged.\n",
    "\n",
    "### 4. **Define Utility Functions**\n",
    "   - Write reusable functions, such as:\n",
    "     - Logging dataset features to MLflow in YAML format.\n",
    "     - Cleaning up files or performing other utility tasks.\n",
    "\n",
    "### 5. **Import Dataset**\n",
    "   - Load the dataset into a DataFrame or a similar structure.\n",
    "   - Ensure that the dataset is prepared for analysis (e.g., by checking data types, null values, etc.).\n",
    "\n",
    "### 6. **Log Original Dataset**\n",
    "   - Log the original/raw dataset to MLflow for traceability and auditing.\n",
    "   - Store this dataset as an artifact for future reference.\n",
    "\n",
    "### 7. **Exploratory Data Analysis (EDA)**\n",
    "   - Perform an EDA to understand data patterns, distributions, and potential outliers.\n",
    "   - Use visualizations and descriptive statistics to explore relationships between features.\n",
    "\n",
    "### 8. **Data Cleaning and Preprocessing**\n",
    "   - Clean and preprocess the dataset:\n",
    "     - Handle missing values, categorical encoding, scaling, etc.\n",
    "     - Prepare the dataset for model training.\n",
    "   - Ensure to keep track of all preprocessing steps for reproducibility.\n",
    "\n",
    "### 9. **Log Transformed Dataset**\n",
    "   - Log the preprocessed/transformed dataset as a separate artifact in MLflow.\n",
    "   - This allows comparison between raw and processed datasets.\n",
    "\n",
    "### 10. **Model Training Pipeline**\n",
    "   - #### **Import Libraries**\n",
    "     - Import all necessary libraries for model training, such as machine learning frameworks (e.g., `scikit-learn`, `TensorFlow`).\n",
    "   \n",
    "   - #### **Define and Log Hyperparameters**\n",
    "     - Set model hyperparameters and log them to MLflow for tracking.\n",
    "\n",
    "   - #### **Train the Model**\n",
    "     - Train the machine learning model using the preprocessed data.\n",
    "    \n",
    "   - #### **Log Metrics**\n",
    "\n",
    "      - Log the training metrics into mlflow\n",
    "\n",
    "   - #### **Log the Model to MLflow**\n",
    "     - Save the trained model to MLflow, ensuring it is accessible for future use or deployment.\n",
    "   \n",
    "   - #### **Log Feature Importance**\n",
    "     - Compute and log feature importance (if applicable) as a visualization (e.g., `.jpg`).\n",
    "     - Store it as an artifact in MLflow for model interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e4732e1-6f23-4758-913a-8f0241ed1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from train import main_train\n",
    "from clean import clean_n_transform\n",
    "from preprocess import preprocess_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca885c-eb21-4c5f-b242-283d8efbea71",
   "metadata": {},
   "source": [
    "### Initiate MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b328ac1c-cd7c-45ff-9a4f-a24fceafbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:6969\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c404cdd-2072-4593-b91d-0c1da1403a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"credit_score_classification_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6c87913-3b6a-4b51-b551-eb79fb8b37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== User-Defined-Function ==========================\n",
    "# Function to log features into features.yaml\n",
    "def log_features_to_yaml(data, dataset_name='Dataset', file_name='features.yaml', description=''):\n",
    "    \"\"\"\n",
    "    Logs the feature names and basic info of a DataFrame into a features.yaml file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The DataFrame containing the features to log.\n",
    "    - dataset_name: A string specifying the name/type of the dataset (e.g., \"Original\" or \"Processed\").\n",
    "    - file_name: The yaml file name.\n",
    "    - description: The description of the set of features (optional).\n",
    "    \"\"\"\n",
    "    # Extract feature names and data types\n",
    "    feature_data_types = data.dtypes.apply(str).to_dict()\n",
    "    \n",
    "    # Prepare the feature info for YAML\n",
    "    feature_info = {\n",
    "        dataset_name: {\n",
    "            'dataset_name': dataset_name,\n",
    "            'features': feature_data_types,\n",
    "            'num_features': len(data.columns),\n",
    "            'description': description\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load existing YAML data if features.yaml already exists\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            existing_data = yaml.safe_load(file) or {}\n",
    "    except FileNotFoundError:\n",
    "        existing_data = {}\n",
    "\n",
    "    # Append the new dataset feature info to the existing data\n",
    "    existing_data.update(feature_info)\n",
    "\n",
    "    # Write updated data back to YAML file\n",
    "    with open(file_name, 'w') as file:\n",
    "        yaml.dump(existing_data, file, default_flow_style=False)\n",
    "\n",
    "    print(f\"Feature information for {dataset_name} saved to {file_name}\")\n",
    "\n",
    "# Function to clean up the features.yaml from the current directory\n",
    "def clean_yaml(file_name='features.yaml'):\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print(f\"{file_name} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"{file_name} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b3928-9aa7-4122-a829-bfd99e23afe2",
   "metadata": {},
   "source": [
    "### Logging Initial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b8d5a-3607-4f8b-a695-fb704e3f6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = './data/train.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffb837-c579-44ee-82ab-72082c5d59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log initial features to yaml\n",
    "log_features_to_yaml(df, \"Initial Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2730fe-0969-4aeb-9ea7-0902a9cff785",
   "metadata": {},
   "source": [
    "### Process and Engineer New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e3bcd07-d766-46a4-abf8-ff7ab9cb5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned, clean_data_path = clean_n_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef49e1-a3b0-49a4-ba0a-d67147397b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log engineered features to yaml\n",
    "log_features_to_yaml(df_cleaned, \"Cleaned Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bc6d6-2d2b-47aa-8931-21f5a7c63241",
   "metadata": {},
   "source": [
    "### Preprocess Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1845780c-8034-45df-bb4a-47dc0c71036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = preprocess_data(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c90ae1-802e-4638-84d7-cfd4ecf7bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and log the preprocessed data used for training\n",
    "train_df = pd.concat([X_train, Y_train], axis=1)\n",
    "log_features_to_yaml(train_df, \"Training Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a155f0-d659-46aa-87b0-7c468411200d",
   "metadata": {},
   "source": [
    "### Train and Log Model and Parameters through `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e69196-3ee7-4faa-a4fa-833e8c75c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End any active MLflow runs\n",
    "if mlflow.active_run() is not None:\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"main-run\") as main_run:\n",
    "    # Log features used within the runs\n",
    "    mlflow.log_artifact(\"features.yaml\")\n",
    "    \n",
    "    # Log cleaning, preprocessing, and training code\n",
    "    mlflow.log_artifact(\"clean.py\")\n",
    "    mlflow.log_artifact(\"preprocess.py\")\n",
    "    mlflow.log_artifact(\"train.py\")\n",
    "    mlflow.log_artifact(\"main.ipynb\")\n",
    "\n",
    "    main_train(X_train, Y_train, X_test, Y_test, tuning_epochs=10, final_epochs=50, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
